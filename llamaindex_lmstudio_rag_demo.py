import requests
from typing import Any, Sequence, AsyncGenerator, Generator

from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.llms import LLM, CompletionResponse, LLMMetadata, ChatMessage
from llama_index.embeddings.huggingface import HuggingFaceEmbedding


class MyLLM(LLM):
    model_name: str
    api_base: str
    api_key: str

    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=4096,
            num_output=512,
            model_name=self.model_name,
            is_chat_model=False,
        )

    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json',
        }
        payload = {
            'model': self.model_name,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': kwargs.get('temperature', 0.7),
        }
        response = requests.post(f'{self.api_base}/v1/chat/completions', json=payload, headers=headers)
        response.raise_for_status()
        text = response.json()['choices'][0]['message']['content']
        return CompletionResponse(text=text)

    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> CompletionResponse:
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json',
        }

        # ğŸ‘‡ ChatMessage ã‚’ dict ã«å¤‰æ›
        openai_messages = [
            {'role': m.role.value, 'content': m.content} for m in messages
        ]

        payload = {
            'model': self.model_name,
            'messages': openai_messages,
            'temperature': kwargs.get('temperature', 0.7),
        }

        response = requests.post(
            f'{self.api_base}/v1/chat/completions',
            json=payload,
            headers=headers,
        )
        response.raise_for_status()
        content = response.json()['choices'][0]['message']['content']

        chat_message = ChatMessage(role='assistant', content=content)
        return CompletionResponse(text=content, message=chat_message)

    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¨éåŒæœŸã¯æœªå¯¾å¿œã®å ´åˆã€NotImplementedErrorã§OK
    def stream_complete(self, prompt: str, **kwargs: Any) -> Generator[CompletionResponse, None, None]:
        raise NotImplementedError('stream_complete is not implemented.')

    def stream_chat(self, messages: Sequence[dict], **kwargs: Any) -> Generator[CompletionResponse, None, None]:
        raise NotImplementedError('stream_chat is not implemented.')

    async def acomplete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        raise NotImplementedError('acomplete is not implemented.')

    async def achat(self, messages: Sequence[dict], **kwargs: Any) -> CompletionResponse:
        raise NotImplementedError('achat is not implemented.')

    async def astream_complete(self, prompt: str, **kwargs: Any) -> AsyncGenerator[CompletionResponse, None]:
        raise NotImplementedError('astream_complete is not implemented.')

    async def astream_chat(self, messages: Sequence[dict], **kwargs: Any) -> AsyncGenerator[CompletionResponse, None]:
        raise NotImplementedError('astream_chat is not implemented.')


def chat(query_engine):
    print()
    print('ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰é–‹å§‹ (çµ‚äº†ã™ã‚‹ã«ã¯ "quit" ã¨å…¥åŠ›)')
    print('-' * 30)

    while True:
        try:
            print()
            user_input = input('ã‚ãªãŸ: ').strip()

            if user_input.lower() in ['quit', 'exit', 'q']:
                print('çµ‚äº†ã—ã¾ã™ã€‚')
                break

            if not user_input:
                continue

            # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
            response = query_engine.query(user_input)

            # çµæœè¡¨ç¤º
            print()
            print('ğŸ§  LLMã‹ã‚‰ã®å›ç­”:')
            print(response.response)

        except KeyboardInterrupt:
            print('\nçµ‚äº†ã—ã¾ã™ã€‚')
            break
        except Exception as e:
            print(f'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}')


def main():
    # âœ… LM Studio ã§èµ·å‹•ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®š
    llm = MyLLM(
        model_name='llama-3-elyza-jp-8b',
        api_base='http://localhost:1234',
        api_key='lm-studio',
    )

    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¯ HuggingFace ã®æ—¥æœ¬èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«
    embed_model = HuggingFaceEmbedding(
        model_name='sonoisa/sentence-bert-base-ja-mean-tokens-v2',
        device='cpu',
    )
    # embed_model = HuggingFaceEmbedding(model_name='cl-tohoku/bert-base-japanese')  # â†fugashiå¿…é ˆ

    # æ–‡æ›¸èª­ã¿è¾¼ã¿
    documents = SimpleDirectoryReader('./rag_data').load_data()

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆllmã¯æ¸¡ã•ãªã„ï¼‰
    index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)

    # LLMä»˜ãã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œæˆ
    query_engine = index.as_query_engine(llm=llm)

    chat(query_engine=query_engine)
    # # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
    # response = query_engine.query('æ´¥å¸‚ã®é°»å±‹ã•ã‚“ã‚’3ã¤ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚')

    # # çµæœè¡¨ç¤º
    # print('ğŸ§  LLMã‹ã‚‰ã®å›ç­”:')
    # print(response.response)


if __name__ == '__main__':
    main()
